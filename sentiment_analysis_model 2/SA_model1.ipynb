{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivh6ZIcRYhs2"
      },
      "source": [
        "## Modelling\n",
        "\n",
        "Creating a TF-IDF vectorizer as an input to a Logistic Regression model to classify the sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyHUC3An-HtJ"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDU7oFK4-KbR",
        "outputId": "408a323e-8925-4121-e2b4-769ec6c163fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32004\n"
          ]
        }
      ],
      "source": [
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER1ozqAm4q5Z"
      },
      "outputs": [],
      "source": [
        "#The input is passed through multiple CNN layers, each with a different kernel size, to capture features at varying n-gram levels.\n",
        "#After each convolution operation, a ReLU activation is applied to introduce non-linearity,\n",
        "#and the extra dimension introduced by Conv2d is removed using squeeze.\n",
        "#Following this, max pooling is performed across the sequence length dimension to extract the most significant features from each filter.\n",
        "#Finally, the outputs from all CNN layers are concatenated along the filter dimension, combining the feature maps learned by each kernel size into a single representation.\n",
        "#Shape: [batch_size, num_filters * len(kernel_sizes)]\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class CNN_LSTM_OvR_Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, kernel_sizes, num_filters, lstm_hidden_dim, num_layers, dropout_rate, glove_weights):\n",
        "        super(CNN_LSTM_OvR_Model, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(glove_weights, requires_grad=True)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embedding_dim))\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=num_filters * len(kernel_sizes),\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout_rate,\n",
        "                            bidirectional=True)\n",
        "\n",
        "        self.fc = nn.Linear(lstm_hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        conv_out = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        conv_out = [torch.max(c, dim=2)[0] for c in conv_out]\n",
        "        conv_out = torch.cat(conv_out, dim=1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(conv_out.unsqueeze(1))\n",
        "        lstm_out = lstm_out.squeeze(1)\n",
        "        drop = self.dropout(lstm_out)\n",
        "        out = self.fc(drop)  # Shape: [batch_size, output_dim]\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "references:https://discuss.pytorch.org/t/cnn-lstm-architecture/151018\n",
        "\n",
        "https://galhever.medium.com/sentiment-analysis-with-pytorch-part-4-lstm-bilstm-model-84447f6c4525\n",
        "\n",
        "https://ieeexplore.ieee.org/abstract/document/8622880"
      ],
      "metadata": {
        "id": "m5mW0bCdFsCQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzjE_40B46gm"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100      # Size of word embeddings\n",
        "hidden_dim = 128         # LSTM hidden size\n",
        "output_dim = 3           # Number of classes (for multi-class classification)\n",
        "kernel_sizes = [3, 4, 5] # Sizes of kernels for CNN\n",
        "num_filters = 100        # Number of filters for CNN\n",
        "lstm_hidden_dim = 128    # LSTM hidden dimension\n",
        "num_layers = 2           # Number of LSTM layers\n",
        "dropout_rate = 0.5       # Dropout rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UklodtHTCC2y"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(glove_path, vocab, embedding_dim=100):\n",
        "    glove_embeddings = {}\n",
        "    with open(glove_path, 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            glove_embeddings[word] = np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "    weights_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "    for i, word in enumerate(vocab):\n",
        "        weights_matrix[i] = glove_embeddings.get(word, np.random.normal(scale=0.6, size=(embedding_dim,)))\n",
        "\n",
        "    return torch.tensor(weights_matrix, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_weights = load_glove_embeddings(\"glove.6B.100d.txt\", vocab, embedding_dim)\n",
        "model = CNN_LSTM_OvR_Model(vocab_size, embedding_dim, hidden_dim, output_dim, kernel_sizes, num_filters, lstm_hidden_dim, num_layers, dropout_rate, glove_weights)\n"
      ],
      "metadata": {
        "id": "sRal6l-I_3ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "04bmKAIO_8Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "H3dwgYy4qn7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBrWhfXP55Fd",
        "outputId": "217a3e75-e965-47e5-91fb-8e564f1275f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded Sequences:\n",
            "tensor([[  436,   545,     0,  ...,     0,     0,     0],\n",
            "        [  279,   641,   353,  ...,     0,     0,     0],\n",
            "        [ 2653,  2574,  1839,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  661,  1915,    38,  ...,     0,     0,     0],\n",
            "        [13200,     1,    39,  ...,     0,     0,     0],\n",
            "        [  685,     8,  1156,  ...,     0,     0,     0]])\n",
            "Labels:\n",
            "tensor([0, 1, 2, 2, 2, 2, 1, 2, 0, 0, 1, 1, 2, 2, 0, 1, 1, 1, 2, 2, 0, 2, 2, 1,\n",
            "        1, 1, 1, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 2, 0, 1, 2, 1, 0, 1,\n",
            "        0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 0, 2, 2, 1, 2, 0, 0, 2, 1, 1, 1, 1, 2, 1,\n",
            "        1, 2, 1, 1, 1, 1, 2, 1, 2, 0, 1, 0, 2, 2, 2, 1, 2, 0, 1, 1, 2, 2, 0, 2,\n",
            "        0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 2, 2, 0, 2, 1, 0, 2, 2, 2, 1, 2, 1, 2, 1,\n",
            "        2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 1, 1, 0, 2,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 2, 1, 2, 0,\n",
            "        1, 1, 2, 1, 2, 2, 2, 1, 0, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2,\n",
            "        2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 2, 2, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 2, 2, 1, 1, 0, 2, 0, 1, 2, 0, 1, 2, 1, 1, 1, 0, 2, 2, 2, 1, 2, 1,\n",
            "        2, 0, 0, 1, 2, 1, 2, 2, 2, 1, 0, 0, 2, 0, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = torch.tensor(self.sequences[idx])\n",
        "        return seq, self.labels[idx]\n",
        "\n",
        "text_dataset = TextDataset(train_df['sequences'].tolist(), train_df['labels'].tolist())\n",
        "\n",
        "train_loader = DataLoader(text_dataset, batch_size=256, shuffle=True, collate_fn=lambda x: (\n",
        "    pad_sequence([item[0] for item in x], batch_first=True),\n",
        "    torch.tensor([item[1] for item in x])\n",
        "))\n",
        "\n",
        "for padded_sequences, labels in train_loader:\n",
        "    print(\"Padded Sequences:\")\n",
        "    print(padded_sequences)\n",
        "    print(\"Labels:\")\n",
        "    print(labels)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC_yFwJN8dMM",
        "outputId": "1e63e4a0-05e3-49e0-e80c-4d9ec70d476c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.552816014458268\n",
            "Epoch 2/10, Loss: 1.302840112153843\n",
            "Epoch 3/10, Loss: 1.158939209423567\n",
            "Epoch 4/10, Loss: 0.9971608271476635\n",
            "Epoch 5/10, Loss: 0.8261752734966886\n",
            "Epoch 6/10, Loss: 0.6675491201332732\n",
            "Epoch 7/10, Loss: 0.5484984625244405\n",
            "Epoch 8/10, Loss: 0.4437979255645559\n",
            "Epoch 9/10, Loss: 0.3629979274114413\n",
            "Epoch 10/10, Loss: 0.31495267204383076\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_LSTM_OvR_Model(\n",
              "  (embedding): Embedding(32004, 100)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
              "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "  )\n",
              "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# Training loop\n",
        "n_epochs = 10\n",
        "batch_size = 256\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        texts = texts.long()\n",
        "        labels = labels.long()\n",
        "\n",
        "        # One-vs-Rest means training binary classifier for each class, so create binary labels for each class i.e. 1 for that class and 0 for the others\n",
        "        for classes in range(3):\n",
        "            binary_labels = (labels == classes).long()\n",
        "            pred = model(texts)\n",
        "            loss = criterion(pred[:, classes], binary_labels.float())  # Compute loss for this class, then backpropagate\n",
        "            loss.backward()\n",
        "            epoch_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss / len(train_loader)}')\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX7wT5dAsTWz"
      },
      "outputs": [],
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] *= 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7a0dip-i3Kw"
      },
      "outputs": [],
      "source": [
        "def map_labels2(label):\n",
        "    if label == 0:\n",
        "        return -1\n",
        "    elif label == 1:\n",
        "        return 0\n",
        "    elif label == 2:\n",
        "        return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QASfx4pPyHeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7116d6c-cf46-4da2-ae3c-85e8d4258852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15, Loss: 0.2643725030598878\n",
            "Epoch 12/15, Loss: 0.2231170985321424\n",
            "Epoch 13/15, Loss: 0.2063102942692771\n",
            "Epoch 14/15, Loss: 0.16214745027629704\n",
            "Epoch 15/15, Loss: 0.1609368107727938\n"
          ]
        }
      ],
      "source": [
        "additional_epochs = 5\n",
        "total_epochs = n_epochs + additional_epochs\n",
        "\n",
        "for epoch in range(n_epochs, total_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        texts = texts.long()\n",
        "        labels = labels.long()\n",
        "\n",
        "        for classes in range(3):\n",
        "            binary_labels = (labels == classes).long()\n",
        "            pred = model(texts)\n",
        "            loss = criterion(pred[:, classes], binary_labels.float())\n",
        "            loss.backward()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{total_epochs}, Loss: {epoch_loss / len(train_loader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbqSRfaDwhh4",
        "outputId": "a5eac69d-13c9-4ee4-e6c9-f79e8de6b290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded Sequences Shape: torch.Size([256, 35])\n",
            "Lengths: tensor([11, 10, 21,  5, 15,  8,  7,  9,  3,  3, 12, 14,  9,  6,  2, 14,  4,  6,\n",
            "         2, 12,  5,  7,  3,  2, 11,  3,  3,  8, 15,  2, 12,  5,  8,  7,  4, 15,\n",
            "         5,  3,  4,  7,  6,  5,  8,  3, 15,  7,  8,  4,  8,  9,  5,  8, 16, 15,\n",
            "         6,  6,  3, 21,  8, 10,  3, 16,  5, 10,  8,  4,  6,  4, 16,  5,  7,  2,\n",
            "        18,  3, 11, 12, 25,  5,  5, 12,  9,  6, 29, 14, 13,  7,  6, 13,  8, 15,\n",
            "         5, 14,  9,  5,  7, 12,  6,  8, 10,  7,  4,  6, 10,  7,  9,  8, 35,  6,\n",
            "        20,  8,  3,  6,  7,  6,  0,  7,  6,  6,  8,  6,  6,  5,  8,  4,  6, 10,\n",
            "         8,  4,  6, 15,  6,  5, 10,  8,  8,  8, 18,  9,  3,  9,  2, 11,  9,  9,\n",
            "        11, 12,  5,  6, 15,  7,  7, 10,  3, 10,  4, 13, 11,  8, 10, 13,  8, 12,\n",
            "         5, 12,  4,  3,  9,  5,  9,  6,  4,  4,  7,  7, 12,  3,  5, 20, 11,  9,\n",
            "         3, 13, 13, 14,  6,  3,  2,  2,  8,  8,  7, 11,  6,  4,  3, 12, 15, 13,\n",
            "         3,  4,  3, 11,  5,  6, 12,  7,  4,  2, 11, 15,  7, 13, 11, 12,  7,  9,\n",
            "         5, 12, 11,  6,  8,  8,  4, 10,  7,  8,  7,  7,  3,  6,  9,  8, 11,  6,\n",
            "         7, 13, 10, 11,  4, 16, 11,  5, 11,  7,  7, 10,  8,  8,  7,  8, 16,  6,\n",
            "         6, 16,  6,  6])\n"
          ]
        }
      ],
      "source": [
        "# code for a pytorch-based pipeline for processing sequences in a dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# TextDataset class is implemented to store and access sequences as PyTorch tensors.\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "      seq = torch.tensor(self.sequences[idx])\n",
        "      return seq\n",
        "\n",
        "\n",
        "# collate_fn function is used during batching to handle varying length sequences by padding them to the same length and calculating their original lengths.\n",
        "def collate_fn(batch):\n",
        "    padded_sequences = pad_sequence([item for item in batch], batch_first=True)\n",
        "    lengths = torch.tensor([len(item) for item in batch])\n",
        "    return padded_sequences, lengths\n",
        "\n",
        "test_dataset = TextDataset(test_df['sequences'].tolist())\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "for padded_sequences, lengths in test_loader:\n",
        "    print(\"Padded Sequences Shape:\", padded_sequences.shape)\n",
        "    print(\"Lengths:\", lengths)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAORCl70h0kE",
        "outputId": "e2a01548-edfa-41f0-8b22-dc5530d36a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, -1, -1, 0, -1, 1, 0, 0, 0, 1, -1, 0, 1, -1, 0, 1, 1, 0, 0, 1, 0, -1, 0, 0, 1, 0, 1, 1, 0, 1, 1, -1, -1, 0, 0, 1, -1, 1, 1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, -1, 1, -1, 0, 1, 0, -1, 1, 0, 0, 1, 1, 0, 1, 1, -1, 0, 0, 1, 0, 1, 1, -1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, -1, 0, 1, 0, 1, 1, 1, -1, 0, 0, -1, 1, 1, 1, 1, 1, -1, -1, -1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, -1, 0, 1, 1, 1, 1, -1, 0, 1, 1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 0, -1, 1, 1, 0, 0, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, 0, 1, -1, 0, -1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, -1, 1, 0, 1, 0, -1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, -1, 0, 1, 0, -1, 0, -1, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 1, 1, 0, -1, 0, 1, 0, -1, 0, 1, 0, 0, 0, -1, -1, 0, 0, 0, 1, 1, 1, -1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, -1, 1, -1, 0, 1, 0, 0, -1, -1, 0, 1, 0, 0, 1, 1, -1, 0, 0, 1, 1, 0, 0, 0, 1, -1, 0, 0, 0, -1, -1, -1, 1, 0, 0, 1, -1, 0, -1, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 0, -1, 0, 1, -1, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 1, 0, -1, 1, -1, 0, 1, 0, -1, 0, 0, 0, 0, 1, -1, 1, 1, 0, 1, -1, 1, -1, -1, 1, 0, 1, -1, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0, -1, -1, 1, 0, -1, 1, 0, -1, 1, 0, 0, 1, -1, -1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, -1, 1, 0, 1, -1, 1, 1, -1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, -1, 1, 0, -1, 0, 0, 1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, -1, 0, 1, -1, 0, 0, 0, 1, 1, 0, -1, 0, 1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 1, 0, -1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, -1, 0, -1, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, 0, -1, 0, 1, 1, -1, -1, 0, 0, 0, 0, -1, -1, -1, 1, -1, 0, 1, 0, 0, -1, 0, 1, 1, 0, 1, 1, 1, -1, 1, 1, -1, 0, 0, -1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, -1, 1, 1, 0, -1, 1, 0, 1, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, 1, -1, 0, -1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, -1, 0, -1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, -1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, -1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 0, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, -1, 1, 1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, -1, 0, -1, 0, 1, 1, -1, 1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 0, 0, -1, 1, -1, 0, 0, 0, 1, 0, 0, -1, 0, 1, -1, 0, -1, 0, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 1, 0, 0, -1, 1, 0, 1, 1, 0, 0, 0, 1, -1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, -1, 0, 0, 1, 1, 0, 0, 0, -1, -1, 0, 0, 1, 1, 0, 0, 1, -1, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 1, 0, -1, -1, 0, 1, 1, 0, 1, 1, -1, 1, 1, 1, 0, 0, -1, 0, 0, 1, 0, 1, 1, 0, 0, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, -1, -1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, -1, -1, 1, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, 0, -1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, -1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -1, 1, 0, -1, -1, 1, 0, 0, 0, 0, 1, 0, 1, 0, -1, 0, 0, 1, 0, 1, 0, -1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 1, -1, 1, 0, 1, -1, 0, 0, -1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 1, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, -1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, -1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, -1, -1, 1, 0, -1, 1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, -1, -1, 1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 1, 0, -1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 1, 1, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, -1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 1, 1, 0, -1, -1, -1, 0, 1, -1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, -1, 1, 1, 0, -1, -1, 0, 0, 0, 0, 1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, 1, 1, -1, 0, 1, -1, -1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, -1, 0, 0, 1, 0, 0, 1, 1, 1, 0, -1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1, 0, -1, 1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, 1, -1, 1, 0, 0, -1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, -1, 1, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, -1, 1, -1, 1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 0, 0, -1, 1, 0, 1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 1, 1, -1, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, -1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, -1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, -1, 0, 1, 1, -1, -1, 1, 1, 1, 0, -1, -1, 1, 1, 0, -1, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, -1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, -1, 1, 1, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 1, 0, 1, 0, 1, -1, -1, 0, 0, 1, 0, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 1, 1, 0, 0, 1, 1, 0, 1, -1, 0, 1, -1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, -1, 0, -1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 0, -1, 1, 0, 1, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, -1, 1, 1, 1, -1, 1, 0, 0, 1, 0, -1, 1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, -1, 1, 0, 0, -1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 0, -1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, -1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 1, -1, 0, 0, -1, 0, 0, 1, 0, 1, 1, 0, -1, 0, 0, 1, 1, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, -1, 0, 1, 0, 1, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, -1, -1, 1, 1, -1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 1, -1, -1, -1, 1, 1, 0, 1, 0, -1, 0, -1, -1, -1, 0, 0, 1, 1, -1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, -1, 0, -1, 0, 0, 0, -1, 0, 1, -1, 0, 1, -1, 0, 0, 1, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, -1, 1, -1, -1, 1, 0, -1, -1, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, -1, -1, -1, 1, -1, 1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 1, 1, 1, 1, -1, 0, 0, 0, 1, 0, 0, -1, -1, 1, 0, 0, 0, 0, -1, -1, 0, 1, 0, -1, 0, 1, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 1, -1, 1, 1, 1, 1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 1, -1, -1, -1, 1, 0, 0, 1, -1, -1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 0, 1, 1, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 0, -1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, -1, 1, 1, -1, 0, 0, -1, 0, 0, 0, 1, 0, 1, 0, 0, -1, 0, 1, 0, 1, -1, 1, -1, 0, 1, 0, 1, 1, 1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 1, 0, -1, 0, 0, 0, -1, 1, 1, 0, -1, 0, -1, 1, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, -1, 1, -1, 1, 1, 1, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, -1, -1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, 0, -1, 1, 0, 0, 1, 0, -1, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 0, 1, 1, 0, -1, 1, 0, 0, 1, 0, -1, 1, 0, 0, 1, 1, 1, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, -1, 0, 1, -1, -1, 0, 1, 0, 0, 1, -1, 0, 1, -1, 1, 0, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, -1, 1, 1, 1, 1, 0, 0, 0, 0, 1, -1, -1, 0, 1, 1, 0, 1, 1, -1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, -1, 1, 0, 1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, -1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, -1, 0, 1, -1, 1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 1, 0, -1, 0, 0, 1, 0, 1, 1, 0, -1, 1, 0, -1, 0, 1, 0, 0, -1, -1, 0, 1, 0, 1, -1, 0, 1, -1, 1, 1, -1, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 1, 0, 1, 1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 0, 0, 0, 1, 0, -1, 1, 0, 1, 0, 1, -1, 0, 0, 1, 0, -1, -1, 0, 0, -1, 0, 1, 1, -1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, -1, -1, 0, 1, 0, 1, 0, -1, 0, 0, -1, 0, 0, 0, 1, -1, -1, 0, 0, 1, 0, 1, -1, 1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 1, 0, -1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, -1, -1, 1, -1, 0, 1, 1, 0, -1, -1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 0, 0, -1, 1, 0, -1, 0, 0, 0, 1, 0, -1, 0, -1, 0, -1, 0, 0, 1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 1, -1, 1, -1, 1, -1, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, -1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, -1, 1, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, 0, 0, 1, 1, 0, 0, -1, -1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 1, -1, 0, 1, -1, 1, 0, 1, 0, 0, 1, 0, 1, -1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, -1, 0, 1, 1, -1, 0, -1, 0, 0, -1, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, -1, 0, 0, 0, 1, -1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, -1, 0, 0, 1, 1, 1, -1, 0, 0, 1, 0, 1, -1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, -1, -1, 1, 1, 0, 1, -1, -1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, -1, 0, 0, 1, -1, -1, 1, -1, 0, 0, -1, 1, 1, -1, 1, 0, 0, -1, -1, -1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, -1, -1, 1, 1, 1, 0, 0, 0, 0, -1, -1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 0, -1, 1, 0, 1, 1, 0, -1, 0, 0, -1, -1, 1, 0, -1, -1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, -1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, -1, 1, -1, -1, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, -1, 1, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, -1, 0, 1, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 1, 0, 0, -1, 0, 1, 0, 0, 0, -1, -1, 0, 1, -1, 1, -1, 0, 0, 0, 0, 1, 0, 1, -1, -1, 0, 1, 0, 0, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, -1, 1, -1, 1, 0, -1, 0, -1, 1, 0, -1, 0, 1, 1, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, -1, 0, 1, 1, 0, 0, 1, 0, -1, -1, -1, 0, 0, 1, -1, 0, 0, 0, 1, 0, 1, 0, 1, -1, 0, 1, 1, 0, 0, -1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, -1, 0, 0, 1, -1, -1, 0, 1, 0, 1, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, 1, -1, -1, 0, 0, 0, 0, -1, 0, 1, -1, 0, -1, 0, 1, 0, 1, -1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 1, 0, 0, 0, -1, 0, 1, 0, 1, 1, 0, 0, 0, 1, -1, 1, -1, 0, 0, 0, 1, 1, 0, 1, -1, 1, -1, -1, 1, 0, 0, 0, -1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, -1, 0, 0, 1, 0, -1, 0, -1, 0, 0, 1, -1, -1, 0, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, -1, -1, 0, 0, 1, 0, -1, 0, -1, -1, 1, 1, 0, -1, 0, 1, 0, -1, 1, 0, 0, 0, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, -1, -1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 0, 1, -1, 0, 0, 0, 1, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, 0, 1, -1, -1, 0, -1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, -1, -1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 1, -1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 1, -1, 0, 0, -1, -1, 1, -1, 0, 1, 0, 0, 0, 1, 1, 1, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 1, -1, 1, 0, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, -1, -1, 0, 1, 0, 0, -1, 1, 0, 1, 0, -1, 0, 0, -1, 0, 0, 1, -1, 1, 0, 0, 1, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 1, 0, -1, 0, 1, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 0, -1, 1, 0, -1, 0, 0, 1, -1, 0, 0, -1, 1, -1, 0, 0, -1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, -1, -1, 1, 0, 0, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, -1, 1, -1, 1, -1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, -1, 1, 1, -1, 0, 0, -1, -1, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, -1, 0, 1, 0, -1, 1, 0, 1, -1, 0, 1, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, -1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 1, -1, -1, -1, 1, 1, -1, -1, 0, 1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 0, -1, 0, -1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 0, 0, -1, 1, -1, -1, -1, -1, 0, 1, 1, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, -1, -1, 0, 1, 1, 1, 1, 1, -1, 1, 0, -1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, -1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, -1, 1, 1, 0, -1, 0, 1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, 1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, 1, 1, 0, 1, -1, -1, 0, 1, 1, -1, 0, 0, 1, -1, 0, 1, 1, -1, 0, -1, 1, 0, 0, 1, 1, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, -1, -1, 0, 0, 0, -1, 0, 1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, -1, 1, -1, 0, 1, 0, 1, 0, -1, 0, 1, 0, 0, -1, 1, 0, 1, 1, 0, 0, -1, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for padded_sequences, lengths in test_loader:\n",
        "        padded_sequences = padded_sequences.long()\n",
        "        predictions = model(padded_sequences)\n",
        "\n",
        "        # torch.argmax to get the predicted class for each sample\n",
        "        predicted_classes = torch.argmax(predictions, dim=1)\n",
        "        mapped_predictions = [map_labels2(pred.item()) for pred in predicted_classes]\n",
        "        all_predictions.extend(mapped_predictions)\n",
        "\n",
        "print(\"Predictions:\", all_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I9BUkgqH-8V"
      },
      "outputs": [],
      "source": [
        "def map_labels2(label):\n",
        "    if label == 0:\n",
        "        return -1\n",
        "    elif label == 1:\n",
        "        return 0\n",
        "    elif label == 2:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kQBFLYaOPIq",
        "outputId": "1a6a1e5d-7baa-4b45-f4bb-2e00b8cbcd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to answer.txt\n"
          ]
        }
      ],
      "source": [
        "predictions_str = \"\\n\".join(map(str, all_predictions))\n",
        "predictions_str = \"\\n\".join(map(str, all_predictions))\n",
        "with open(\"answer2.txt\", \"w\") as f:\n",
        "    f.write(predictions_str)\n",
        "print(\"Predictions saved to answer.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoZF3Ep-75IT"
      },
      "outputs": [],
      "source": [
        "#https://ieeexplore.ieee.org/abstract/document/8622880"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_cuda_py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}